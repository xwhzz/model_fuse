diff --git a/include/onnxruntime/core/framework/tensor_shape.h b/include/onnxruntime/core/framework/tensor_shape.h
index 82a1c1de83..173815fffb 100644
--- a/include/onnxruntime/core/framework/tensor_shape.h
+++ b/include/onnxruntime/core/framework/tensor_shape.h
@@ -112,6 +112,8 @@ class TensorShape {
    */
   int64_t Size() const;
 
+  int64_t GoodSize() const;
+
   /**
      Return the total number of elements up to the specified dimension.
      If the dimension interval is empty (dimension == 0), return 1.
diff --git a/onnxruntime/core/framework/tensor_shape.cc b/onnxruntime/core/framework/tensor_shape.cc
index 399dc1a2a4..124ce14d29 100644
--- a/onnxruntime/core/framework/tensor_shape.cc
+++ b/onnxruntime/core/framework/tensor_shape.cc
@@ -60,6 +60,12 @@ int64_t TensorShape::Size() const {
   return size;
 }
 
+int64_t TensorShape::GoodSize() const {
+  int64_t size = SizeHelper(1, values_.size());
+  // should we cache the size? as multiple operation may be expensive.
+  return size;
+}
+
 int64_t TensorShape::SizeToDimension(size_t dimension) const {
   const size_t num_dims = values_.size();
   ORT_ENFORCE(dimension <= num_dims,
diff --git a/onnxruntime/core/graph/graph.cc b/onnxruntime/core/graph/graph.cc
index 0c1d79532f..42a71c9010 100644
--- a/onnxruntime/core/graph/graph.cc
+++ b/onnxruntime/core/graph/graph.cc
@@ -2867,13 +2867,27 @@ Status Graph::InferAndVerifyTypeMatch(Node& node, const OpSchema& op, const Reso
       // This should not happen: indicates incompleteness in ONNX inference.
       std::stringstream ss;
       ss << "index=" << operand_index;
+      bool flag = false;
       for (auto it = op_formal_parameter.GetTypes().begin(); it != op_formal_parameter.GetTypes().end(); ++it) {
         ss << "," << *(*it);
+        std::string type = *(*it);
+        if ((type == "tensor(float)") && ( "Route" == node.OpType() || ("Merge" == node.OpType() && output_def->Name().find("_sum") != std::string::npos))) {
+          inferred_type = *it;
+          flag = true;
+          break;
+        }
+        else if (type == "tensor(int32)" && "Merge" == node.OpType() && output_def->Name().find("_add") != std::string::npos) {
+          inferred_type = *it;
+          flag = true;
+          break;
+        }
       }
-      Status status(ONNXRUNTIME, onnxruntime::common::StatusCode::FAIL,
+      if (!flag){
+        Status status(ONNXRUNTIME, onnxruntime::common::StatusCode::FAIL,
                     "Node (" + node_name + ") Op (" + node.OpType() + ") output arg (" +
                         output_def->Name() + ") type inference failed, inferred types: " + ss.str());
-      return status;
+        return status;
+      }
     }
 
     if ((existing_type != inferred_type) && (existing_type != nullptr)) {
diff --git a/onnxruntime/core/providers/cpu/tensor/reshape_helper.h b/onnxruntime/core/providers/cpu/tensor/reshape_helper.h
index d7ceda16e6..44bb8c4ce4 100644
--- a/onnxruntime/core/providers/cpu/tensor/reshape_helper.h
+++ b/onnxruntime/core/providers/cpu/tensor/reshape_helper.h
@@ -18,6 +18,7 @@ class ReshapeHelper {
     auto nDims = requested_shape.size();
     ptrdiff_t unknown_dim = -1;
     int64_t size = 1;
+    int64_t good_size = 1;
     for (size_t i = 0; i < nDims; ++i) {
       ORT_ENFORCE(requested_shape[i] >= -1, "A dimension cannot be less than -1, got ", requested_shape[i]);
       if (requested_shape[i] == -1) {
@@ -30,16 +31,23 @@ class ReshapeHelper {
                       " the dimension size of the input tensor.");
           requested_shape[i] = input_shape[i];
         }
+        if (requested_shape[i] !=0){
+          good_size *= requested_shape[i];
+        }
         size *= requested_shape[i];
       }
     }
 
     if (unknown_dim != -1) {
       // calculate unknown dimension
-      ORT_ENFORCE(size != 0 && (input_shape_size % size) == 0,
-                  "The input tensor cannot be reshaped to the requested shape. Input shape:", input_shape,
-                  ", requested shape:", TensorShape(requested_shape));
-      requested_shape[unknown_dim] = input_shape_size / size;
+      if (input_shape[0] == 0 && unknown_dim != 0) {
+        requested_shape[unknown_dim] = input_shape.GoodSize() / good_size;
+      } else {
+        ORT_ENFORCE(size != 0 && (input_shape_size % size) == 0,
+                    "The input tensor cannot be reshaped to the requested shape. Input shape:", input_shape,
+                    ", requested shape:", TensorShape(requested_shape));
+        requested_shape[unknown_dim] = input_shape_size / size;
+      }
     } else {
       // check if the output shape is valid.
       ORT_ENFORCE(input_shape_size == size,
diff --git a/onnxruntime/test/testdata/custom_op_library/cpu/cpu_ops.cc b/onnxruntime/test/testdata/custom_op_library/cpu/cpu_ops.cc
index ebef441350..933083ac45 100644
--- a/onnxruntime/test/testdata/custom_op_library/cpu/cpu_ops.cc
+++ b/onnxruntime/test/testdata/custom_op_library/cpu/cpu_ops.cc
@@ -6,6 +6,7 @@
 #undef ORT_API_MANUAL_INIT
 
 #include "onnxruntime_lite_custom_op.h"
+#include <iostream>
 
 #define CUSTOM_ENFORCE(cond, msg)                                \
   if (!(cond)) {                                                 \
@@ -175,6 +176,65 @@ void Box(const Ort::Custom::Tensor<float>* float_in_1,
   }
 }
 
+Ort::Status Merge(const Ort::Custom::TensorArray& inputs, Ort::Custom::TensorArray& outputs) {
+
+    auto output_shape = inputs[0]->Shape();
+    output_shape[0] = 0;
+    for (size_t ith_input = 0; ith_input < inputs.Size(); ++ith_input) {
+        output_shape[0] += inputs[ith_input]->Shape()[0];
+    }
+    float* raw_output_0 = outputs.AllocateOutput<float>(0, output_shape);
+    
+    // 分配索引tensor
+    int32_t* raw_output_1 = outputs.AllocateOutput<int32_t>(1, {inputs.Size() + 1});
+    size_t index = 0;
+    size_t start = 0;
+    raw_output_1[0] = 0; // 第一个索引总是从0开始
+
+    // 按顺序复制每个输入tensor的数据到输出tensor
+    for (size_t ith_input = 0; ith_input < inputs.Size(); ++ith_input) {
+        const auto& input = inputs[ith_input];
+        const float* raw_input = reinterpret_cast<const float*>(input->DataRaw());
+        size_t num_elements = input->NumberOfElement();
+
+        // 更新索引tensor
+        index += input->Shape()[0];
+        raw_output_1[ith_input + 1] = static_cast<int32_t>(index);
+
+        // 复制数据
+        memcpy(raw_output_0 + start, raw_input, num_elements * sizeof(float));
+        start += num_elements;
+    }
+
+    return Ort::Status{nullptr};
+}
+
+
+Ort::Status Route(const Ort::Custom::Tensor<float>& data_tensor,
+                  const Ort::Custom::Tensor<int32_t>& index_tensor,
+                  Ort::Custom::TensorArray& outputs) {
+    const float* data_raw = data_tensor.Data();
+    const int32_t* index_raw = index_tensor.Data();
+    int64_t num_outputs = index_tensor.NumberOfElement() - 1;
+    int64_t scale = data_tensor.NumberOfElement() / data_tensor.Shape()[0];
+
+    for (int64_t i = 0; i < num_outputs; ++i) {
+        int64_t start_index = index_raw[i];
+        int64_t end_index = index_raw[i + 1];
+        int64_t num_elements = (end_index - start_index) * scale;
+
+        // Assume that all tensors have the same shape except for the batch dimension
+        std::vector<int64_t> output_shape = data_tensor.Shape();
+        output_shape[0] = end_index - start_index;
+
+        float* raw_output = outputs.AllocateOutput<float>(i, output_shape);
+        memcpy(raw_output, data_raw + start_index * scale, num_elements * sizeof(float));
+    }
+
+    return Ort::Status{nullptr};
+}
+
+
 #if !defined(DISABLE_FLOAT8_TYPES)
 struct KernelOneFloat8 {
   void Compute(OrtKernelContext* context) {
@@ -331,6 +391,9 @@ void RegisterOps(Ort::CustomOpDomain& domain) {
   static const std::unique_ptr<OrtLiteCustomOp> c_Select{Ort::Custom::CreateLiteCustomOp("Select", "CPUExecutionProvider", Select)};
   static const std::unique_ptr<OrtLiteCustomOp> c_Filter{Ort::Custom::CreateLiteCustomOp<Filter>("Filter", "CPUExecutionProvider", 15, 17)};
   static const std::unique_ptr<OrtLiteCustomOp> c_Box{Ort::Custom::CreateLiteCustomOp("Box", "CPUExecutionProvider", Box)};
+
+  static const std::unique_ptr<OrtLiteCustomOp> c_Merge{Ort::Custom::CreateLiteCustomOp("Merge", "CPUExecutionProvider", Merge)};
+  static const std::unique_ptr<OrtLiteCustomOp> c_Route{Ort::Custom::CreateLiteCustomOp("Route", "CPUExecutionProvider", Route)};
   static const std::unique_ptr<OrtLiteCustomOp> c_CopyTensorArrayAllVariadic{Ort::Custom::CreateLiteCustomOp("CopyTensorArrayAllVariadic", "CPUExecutionProvider", CopyTensorArrayAllVariadic<float>)};
   static const std::unique_ptr<OrtLiteCustomOp> c_CopyTensorArrayCombined{Ort::Custom::CreateLiteCustomOp("CopyTensorArrayCombined", "CPUExecutionProvider", CopyTensorArrayCombined<float>)};
 
@@ -353,6 +416,8 @@ void RegisterOps(Ort::CustomOpDomain& domain) {
   domain.Add(c_CopyTensorArrayAllVariadic.get());
   domain.Add(c_CopyTensorArrayCombined.get());
   domain.Add(c_AtterTesterIntFloat.get());
+  domain.Add(c_Merge.get());
+  domain.Add(c_Route.get());
   domain.Add(&c_AtterTesterString);
 
 #if !defined(DISABLE_FLOAT8_TYPES)
